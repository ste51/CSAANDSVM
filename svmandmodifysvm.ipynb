{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9419780219780219\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "# 加载Wine Quality数据集\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# 将质量等级分为三类：低质量（1-4）、中等质量（5-6）和高质量（7-10）\n",
    "data['quality'] = data['quality'].apply(lambda x: 0 if x <= 4 else (1 if x <= 6 else 2))\n",
    "\n",
    "# 分离特征和标签\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 使用带有RBF核的SVM\n",
    "svm = SVC(kernel='rbf', C=3, gamma=2)\n",
    "\n",
    "# 训练模型\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上预测\n",
    "y_pred = svm.predict(X_test)\n",
    "labels = list(range(0, 3))  # 就是y中有多少种分类，就给多少个标签\n",
    "# 计算准确率\n",
    "test_accuracy = metrics.precision_score(y_test, y_pred, labels=labels, average='macro', zero_division=1)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始前形状： (100, 2)\n",
      "开始前形状： (200, 2)\n",
      "fitness is [0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94982079 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94982079 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94982079 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94982079 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94982079 0.94921316 0.94921316 0.94921316 0.94921316\n",
      " 0.94921316 0.94921316 0.94921316 0.94921316 0.94097222 0.94921316\n",
      " 0.94097222 0.94097222 0.94097222 0.94921316 0.94097222 0.94097222\n",
      " 0.94097222 0.94097222 0.94097222 0.94097222 0.94921316 0.94097222\n",
      " 0.94097222 0.94097222 0.94982079 0.94097222 0.94097222 0.94097222\n",
      " 0.94097222 0.94921316 0.94097222 0.94097222 0.94097222 0.94982079\n",
      " 0.94097222 0.94097222 0.94921316 0.94097222 0.94097222 0.94097222\n",
      " 0.94921316 0.94097222 0.94097222 0.94097222 0.94097222 0.94921316\n",
      " 0.94097222 0.94097222 0.94097222 0.94921316 0.94097222 0.94097222\n",
      " 0.94097222 0.94097222 0.94097222 0.94097222 0.94921316 0.94097222\n",
      " 0.94097222 0.94097222 0.94921316 0.94097222 0.94097222 0.94097222\n",
      " 0.94097222 0.94921316 0.94097222 0.94097222 0.94097222 0.94921316\n",
      " 0.94097222 0.94097222 0.94921316 0.94097222 0.94097222 0.94097222\n",
      " 0.94921316 0.94097222 0.94097222 0.94097222 0.94097222 0.94921316\n",
      " 0.94097222 0.94097222 0.94097222 0.94921316 0.94097222 0.94097222\n",
      " 0.94097222 0.94097222 0.94097222 0.94097222 0.94097222 0.94097222\n",
      " 0.94921316 0.94097222 0.94097222 0.94097222 0.94097222 0.94097222\n",
      " 0.94097222 0.94921316 0.94097222 0.94097222 0.94097222 0.94921316\n",
      " 0.94097222 0.94097222]\n",
      "最好的fitness是： 0.949820788530466\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 75.38040073375166\n",
      "γ: 3.719544529628404\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 1.8807168580314602\n",
      "γ: 3.033206315722379\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 1.8807168580314602\n",
      "γ: 3.033206315722379\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 1.8807168580314602\n",
      "γ: 3.033206315722379\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 1.8807168580314602\n",
      "γ: 3.033206315722379\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 8.501893945754777\n",
      "γ: 3.6976682109740686\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 8.501893945754777\n",
      "γ: 3.6976682109740686\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 8.501893945754777\n",
      "γ: 3.6976682109740686\n",
      "最好的fitness是： 0.9528985507246377\n",
      "C: 8.501893945754777\n",
      "γ: 3.6976682109740686\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 2.346865017986075\n",
      "γ: 2.6220450239582296\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 68.62808073953956\n",
      "γ: 2.823312144633848\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 68.62808073953956\n",
      "γ: 2.823312144633848\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 63.860382830292075\n",
      "γ: 2.5589106495839515\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 71.6817381208218\n",
      "γ: 2.7887754532417404\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 71.6817381208218\n",
      "γ: 2.7887754532417404\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 71.6817381208218\n",
      "γ: 2.7887754532417404\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 71.6817381208218\n",
      "γ: 2.7887754532417404\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 71.6817381208218\n",
      "γ: 2.7887754532417404\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 159.46610920461947\n",
      "γ: 2.688298026907923\n",
      "最好的fitness是： 0.9535221496005809\n",
      "C: 159.46610920461947\n",
      "γ: 2.688298026907923\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.59968682984598\n",
      "γ: 2.3535740481778453\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.54290221464849\n",
      "γ: 2.354545414804036\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 90.54290221464849\n",
      "γ: 2.354545414804036\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 91.93101565061995\n",
      "γ: 2.360840496604192\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "最好的fitness是： 0.9547775346462437\n",
      "C: 92.5498502450204\n",
      "γ: 2.3602101669621636\n",
      "Optimized SVM training accuracy: 1.0\n",
      "Optimized SVM testing accuracy: 0.9547775346462437\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "def elite_retention(crows, fitness):\n",
    "    # 找到当前最优解\n",
    "    best_index = np.argmax(fitness)\n",
    "    gbest = crows[best_index]\n",
    "    print('最好的fitness是：',fitness[best_index])\n",
    "    return gbest\n",
    "\n",
    "def adaptive_step(current_iter, max_iter, fl_max, fl_min):\n",
    "    # 计算当前迭代的飞行步长\n",
    "    #fl = fl_max - (fl_max - fl_min) * (2 * (current_iter / max_iter) - (current_iter / max_iter)**2)\n",
    "    fl = fl_min + (fl_max - fl_min) * (2 * (current_iter / max_iter) - (current_iter / max_iter)**2)\n",
    "    \n",
    "    return fl\n",
    "\n",
    "def is_prime(n):\n",
    "    \"\"\"判断一个数是否为素数\"\"\"\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n)) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def init_pop(pop_size, dimension, bounds):\n",
    "    \"\"\"使用佳点集方法生成初始种群\"\"\"\n",
    "    p = np.zeros((pop_size, dimension))\n",
    "    prime_number_min = dimension * 2 + 3\n",
    "    # 找到(prime_number_min-3)/2>=dimension的最小素数prime_number_min\n",
    "    while not is_prime(prime_number_min):\n",
    "        prime_number_min += 1\n",
    "    \n",
    "    #由于随机生成的方式，可能会出现某些维度上的值并不满足预设的边界条件，尤其是当边界非常狭窄时（例如，你的第二个维度的范围是从 0.00001 到 1）。\n",
    "    for i in range(pop_size):\n",
    "        for j in range(dimension):\n",
    "            r = np.mod(2 * np.cos(2 * np.pi * (j + 1) / prime_number_min) * (i + 1), 1)\n",
    "            p[i, j] = bounds[j, 0] + r * (bounds[j, 1] - bounds[j, 0])\n",
    "    return p\n",
    "\n",
    "def OBL(crows,top,low,pop_size,dimension):\n",
    "    # print(top + low )\n",
    "    # print('开始前：',crows)\n",
    "    print('开始前形状：', crows.shape)\n",
    "    tempcrow = np.zeros((pop_size, dimension))\n",
    "    for i in range(pop_size):\n",
    "        tempcrow[i] = top + low - crows[i]\n",
    "    \n",
    "    # 将 tempcrow 添加到 crows 的末尾\n",
    "    crows = np.vstack((crows, tempcrow))\n",
    "    # 确保所有值都在合法范围内\n",
    "    crows = np.clip(crows, bounds[:, 0], bounds[:, 1])\n",
    "    # print('之后：',crows)\n",
    "    print('开始前形状：', crows.shape)\n",
    "    # exit(0)\n",
    "    return crows\n",
    "    # return top + low -crows\n",
    "# 定义计算函数\n",
    "def APFunction(t,T,AP_max , AP_min, n, p, alpha=0, beta=1):\n",
    "    return AP_min + (AP_max-AP_min)*(1 - (t / T)**n * (1 + alpha * np.sin(np.pi * (t / T)**p) ** beta))\n",
    "def crow_search_algorithm(num_crows, num_dimensions, bounds,max_iter, AP,AP_max,AP_min,alpha , fl_max, fl_min, X_train, y_train):\n",
    "    # # 初始化乌鸦位置（修改初始化范围和采用对数均匀分布）\n",
    "    # crows = np.zeros((num_crows, num_dimensions))\n",
    "    # for i in range(num_crows):\n",
    "    #     crows[i][0] = np.exp(np.random.uniform(np.log(1), np.log(100000)))\n",
    "    #     crows[i][1] = np.exp(np.random.uniform(np.log(0.000001), np.log(1)))\n",
    "    \n",
    "    # # 定义种群大小\n",
    "    # pop_size = 100\n",
    "    # # 定义种群的取值范围\n",
    "    # dimension_2 = 2\n",
    "    # bounds_2 = np.array([[0, 100], [0.001, 1]])\n",
    "\n",
    "    #二维、三维的佳点集种群\n",
    "    crows = init_pop(num_crows, num_dimensions, bounds)\n",
    "    \n",
    "    crows = OBL(crows, np.array([100,100]), np.array([0,0]), num_crows, num_dimensions)\n",
    "    \n",
    "    # print(crows)\n",
    "    # exit(0)\n",
    "    # crows_temp = np.copy(crows)\n",
    "    fitness = np.zeros(num_crows * 2)\n",
    "    \n",
    "    for i in range(num_crows * 2):\n",
    "        # 计算每个乌鸦的适应度值\n",
    "        # 使用径向基函数(rbf)作为核，初始化SVC模型\n",
    "        # 其中C参数和gamma参数通过crows数组获取\n",
    "        svc = SVC(kernel='rbf',C=crows[i][0], gamma=crows[i][1])\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_test_pred = svc.predict(X_test)\n",
    "        # 将结果存储在fitness数组的第i个位置\n",
    "        fitness[i] = metrics.precision_score(y_test, y_test_pred, labels=labels, average='macro', zero_division=1)\n",
    "    print('fitness is',fitness)\n",
    "    \n",
    "    # 精英保留\n",
    "    gbest = elite_retention(crows, fitness)\n",
    "    t = 0\n",
    "    AP_n = 2\n",
    "    AP_p = 1\n",
    "    AP_alpha=0.5\n",
    "    AP_beta=2\n",
    "    for t in range(max_iter):\n",
    "        fl = adaptive_step(t, max_iter, fl_max, fl_min)\n",
    "        \n",
    "        AP = APFunction(t, max_iter,AP_max,AP_min, AP_n, AP_p, AP_alpha, AP_beta)\n",
    "        for i in range(num_crows * 2):\n",
    "            r1 = np.random.rand()\n",
    "            r2 = np.random.rand()\n",
    "            # print('cros is ',crows[i])\n",
    "            crows_temp = np.zeros(crows[i].shape)\n",
    "            if r1 >= AP:\n",
    "                # 更新乌鸦位置\n",
    "                crows_temp = crows[i] + r2 * fl * (gbest - crows[i])\n",
    "                # print('best cros_temp is ',crows_temp)\n",
    "            else:\n",
    "                crows_temp[0] = np.exp(np.random.uniform(np.log(bounds[0, 0]), np.log(bounds[0, 1])))\n",
    "                crows_temp[1] = np.exp(np.random.uniform(np.log(bounds[1, 0]), np.log(bounds[1, 1])))\n",
    "                # print('random cros_temp is ',crows_temp)\n",
    "            crows_temp = np.clip(crows_temp, bounds[:, 0], bounds[:, 1])\n",
    "            # print('cros_temp is ',crows_temp)\n",
    "            # 计算新的适应度值\n",
    "            svc = SVC(kernel='rbf',C=crows_temp[0] , gamma=crows_temp[1])\n",
    "            # scores = cross_val_score(svc, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            svc.fit(X_train, y_train)\n",
    "            y_test_pred = svc.predict(X_test)\n",
    "            # 将结果存储在fitness数组的第i个位置\n",
    "            new_fitness = metrics.precision_score(y_test, y_test_pred, labels=labels, average='macro', zero_division=1)\n",
    "            \n",
    "            # 如果新位置更好，则更新\n",
    "            if new_fitness > fitness[i]:\n",
    "                fitness[i] = new_fitness\n",
    "                crows[i] = crows_temp\n",
    "        \n",
    "        # 更新全局最优解\n",
    "        gbest = elite_retention(crows, fitness)\n",
    "        print(\"C:\", gbest[0])\n",
    "        # t = t + 1\n",
    "        print(\"γ:\", gbest[1])\n",
    "    # 返回最优参数\n",
    "    best_params = {'kernel':'rbf','C': gbest[0], 'gamma': gbest[1]}\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "# 加载Wine Quality数据集\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# # 将质量等级分为三类：低质量（1-4）、中等质量（5-6）和高质量（7-10）\n",
    "data['quality'] = data['quality'].apply(lambda x: 0 if x <= 4 else (1 if x <= 6 else 2))\n",
    "\n",
    "# 分离特征和标签\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "#控制随机种子：random_state 参数用于设置随机数生成器的种子。这意味着每次运行代码时，如果 random_state 的值相同，数据集的分割结果将会一致。\n",
    "# 可重复性：通过设置 random_state，可以确保实验的可重复性。这对于调试和验证模型非常重要，因为你可以确保每次运行代码时使用的训练集和测试集是相同的。\n",
    "# 默认值：如果 random_state 未设置（即为 None），train_test_split 函数将使用一个随机的种子值，这会导致每次运行代码时数据集的分割结果不同。\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# # 特征选择\n",
    "# svc = SVC(kernel='linear')\n",
    "# selector = RFE(svc, n_features_to_select=5, step=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# X_train_selected = selector.transform(X_train)\n",
    "# X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# 参数设置\n",
    "num_crows = 100\n",
    "num_dimensions = 2  # C和gamma两个参数\n",
    "bounds = np.array([[1e-10, 200], [1e-10, 200]]) #两个参数分别的范围,把0变成1e-10是为了防止对数np.log引发除零错误。\n",
    "\n",
    "max_iter = 100\n",
    "AP = 0.2\n",
    "AP_max = 1\n",
    "AP_min = 0.01\n",
    "\n",
    "alpha = 0.5  # CSA 参数\n",
    "fl_max = 2\n",
    "fl_min = 0.01\n",
    "labels = list(range(0, 3))  # 就是y中有多少种分类，就给多少个标签\n",
    "\n",
    "# 运行乌鸦搜索算法\n",
    "best_params = crow_search_algorithm(num_crows, num_dimensions, bounds, max_iter, AP, AP_max, AP_min, alpha, fl_max, fl_min, X_train, y_train)\n",
    "# best_params = {'kernel':'rbf','C': 1, 'gamma': 100}\n",
    "# 使用最优参数训练SVM模型\n",
    "svc = SVC(**best_params)\n",
    "svc.fit(X_train, y_train)\n",
    "y_train_pred = svc.predict(X_train)\n",
    "y_test_pred = svc.predict(X_test)\n",
    "\n",
    "\n",
    "# print(y_test_pred)\n",
    "# 评估模型\n",
    "#train_scores = cross_val_score(svc, X_train, y_train, cv=5, scoring='accuracy') 这个方式准确率太低了，所以就算了\n",
    "train_scores = metrics.precision_score(y_train, y_train_pred, labels=labels, average='macro', zero_division=1)\n",
    "test_scores = metrics.precision_score(y_test, y_test_pred, labels=labels, average='macro', zero_division=1)\n",
    "\n",
    "print(\"Optimized SVM training accuracy:\", train_scores)\n",
    "print(\"Optimized SVM testing accuracy:\", test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLPOLICYGRADIENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
